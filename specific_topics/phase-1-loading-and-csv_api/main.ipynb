{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "Step-by-Step Explanation\n",
    "    \n",
    "    # Part A: Working with CSV Files\n",
    "    CSVs are everywhere in data pipelines. Key challenges:\n",
    "\n",
    "    Large files (GBs) that don't fit in memory\n",
    "    Messy data (wrong delimiters, encoding issues, headers)\n",
    "    Type inference (dates as strings, numbers as text)\n",
    "\n",
    "    # Part B: Fetching Data from APIs\n",
    "\n",
    "    APIs return JSON typically. You need to:\n",
    "\n",
    "    Make HTTP requests\n",
    "    Parse nested JSON structures\n",
    "    Convert to DataFrame for manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "path = Path(\"../../csv/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all things combined\n",
    "\n",
    "# ENGINEERING APPROACH: Specify parameters for reliability\n",
    "df = pd.read_csv(path / \"user_data.csv\",\n",
    "    sep=',',                    # Explicit delimiter\n",
    "    encoding='utf-8',           # Handle special characters\n",
    "    parse_dates=['signup_date'], # Auto-convert date columns\n",
    "    dtype={'user_id': str},     # Force specific types\n",
    "    na_values=['NULL', 'N/A']   # Custom null indicators)\n",
    ")\n",
    "print(df.head(3))\n",
    "print(f\"\\nShape: {df.shape}\")  # (rows, columns)\n",
    "print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "# Universal parsing pattern (memorize this)\n",
    "\n",
    "    * Load as string\n",
    "\n",
    "    * Parse with errors=\"coerce\"\n",
    "\n",
    "    * Compare parsed vs raw\n",
    "\n",
    "    Classify:\n",
    "\n",
    "        * Missing\n",
    "\n",
    "        * Invalid\n",
    "\n",
    "        * Valid\n",
    "\n",
    "    This pattern is type-agnostic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "# working on delimiter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "<!-- practise data for delimiter -->\n",
    "\n",
    "user_id,name,signup_date,age\n",
    "001,John Doe,2024-01-15,29\n",
    "002,Jane Smith,2024-02-20,34\n",
    "003,Alice Brown,2024-03-05,27\n",
    "\n",
    "user_id|name|signup_date|age\n",
    "001|John Doe|2024-01-15|29\n",
    "002|Jane Smith|2024-02-20|34\n",
    "003|Alice Brown|2024-03-05|27\n",
    "\n",
    "\n",
    "user_id;name;signup_date;age\n",
    "001;John Doe;2024-01-15;29\n",
    "002;Jane Smith;2024-02-20;34\n",
    "003;Alice Brown;2024-03-05;27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(path /'user_data.csv',sep=',')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(path /'user_data.csv',sep='|')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(path /'user_data.csv',sep=';')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "# working on encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(path /\"user_utf_8.csv\", encoding=\"utf-8\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(path /\"user_latin_1.csv\", encoding=\"utf-8\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "# working on parse_dates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "Missing and invalid dates become indistinguishable (NaT). with parse dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here the the dates are not converted to datetime but object\n",
    "df = pd.read_csv(\n",
    "    path /\"user_dates.csv\",\n",
    "    parse_dates=[\"signup_date\"],\n",
    "    na_values=[\"\", \"N/A\"]\n",
    ")\n",
    "\n",
    "df['signup_date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# working on the above problem and converting them explicitly\n",
    "# donot combine use pd.to_datetime where we explicitly force our data to date\n",
    "df = pd.read_csv(\n",
    "    path /\"user_dates.csv\",\n",
    "    parse_dates=[\"signup_date\"],\n",
    ")\n",
    "\n",
    "\n",
    "df['updated_date'] = pd.to_datetime(\n",
    "    df['signup_date'],\n",
    "    errors='coerce'\n",
    ")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['updated_date'].isna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  CLASSIFY THE DATA AS INVALID ETC\n",
    "is_missing = df[\"signup_date\"].isna()\n",
    "is_invalid = df['updated_date'].isna() & df[\"signup_date\"].notna()\n",
    "is_valid = df['updated_date'].notna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[is_missing]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[is_invalid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[is_valid]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "# working with specific dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "✅ IDs / identifiers\n",
    "\n",
    "d   type={'user_id': str}\n",
    "\n",
    "\n",
    "    Also applies to:\n",
    "\n",
    "        order_id\n",
    "\n",
    "        customer_id\n",
    "\n",
    "        account_id\n",
    "\n",
    "        employee_code\n",
    "\n",
    "    Any column that:\n",
    "\n",
    "        Looks numeric\n",
    "\n",
    "        Must preserve leading zeros\n",
    "\n",
    "        Is never used in arithmetic\n",
    "\n",
    "    Why:\n",
    "\n",
    "        Prevents 001 → 1\n",
    "\n",
    "        Prevents float coercion\n",
    "\n",
    "        Prevents silent corruption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_dtype = {\n",
    "    'user_id': str,\n",
    "    'zip_code': str,\n",
    "    'gender': 'category',\n",
    "    'plan_type': 'category'\n",
    "}\n",
    "\n",
    "df = pd.read_csv(path /'user_specific_dtypes.csv', dtype=d_dtype)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "Rule of thumb: Only convert columns to numeric if you intend to calculate on them. Otherwise, keep as string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize numeric and datetime columns\n",
    "df['signup_date'] = pd.to_datetime(df['signup_date'], errors='coerce')\n",
    "df['age'] = pd.to_numeric(df['age'], errors='coerce')\n",
    "df['salary'] = pd.to_numeric(df['salary'], errors='coerce')\n",
    "\n",
    "# Separate missing vs invalid dates\n",
    "missing_dates = df['signup_date'].isna() & df['signup_date'].isna()\n",
    "invalid_dates = df['signup_date'].notna() & df['signup_date'].isna()\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "# working on na_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "na_values=['NULL', 'N/A']\n",
    "What it does\n",
    "\n",
    "When pandas reads a CSV, it normally treats some things as NaN automatically, e.g.:\n",
    "\n",
    "\"\"  → NaN\n",
    "\"NaN\" → NaN\n",
    "\n",
    "\n",
    "na_values extends this list with custom strings that should also be considered missing.\n",
    "\n",
    "In your example:\n",
    "\n",
    "na_values=['NULL', 'N/A']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "Key points to remember\n",
    "\n",
    "Always include all variants of missing data in production pipelines.\n",
    "\n",
    "Common: \"\", \"N/A\", \"NULL\", \"None\", \"na\", \"NA\", \"–\"\n",
    "\n",
    "This is applied before parsing, so it affects:\n",
    "\n",
    "parse_dates\n",
    "\n",
    "pd.to_numeric\n",
    "\n",
    "Missing ≠ invalid\n",
    "\n",
    "NaN / NaT → missing\n",
    "\n",
    "Garbage / wrong format → invalid (needs explicit detection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from io import StringIO\n",
    "\n",
    "data = \"\"\"\n",
    "user_id,signup_date,age,salary\n",
    "001,2024-01-15,29,50000\n",
    "002,,34,62000\n",
    "003,2024-02-30,27,45000\n",
    "004,N/A, ,70000\n",
    "005,2024-13-01,31,not_available\n",
    "006,garbage,28,58000\n",
    "\"\"\"\n",
    "\n",
    "csv = StringIO(data)\n",
    "df = pd.read_csv(\n",
    "    csv,\n",
    "    na_values=['NULL', 'N/A', ' ', 'not_available']\n",
    ")\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "# LOADING LARGE CHUNK OF DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "\n",
    "# Constants\n",
    "num_rows = 50000\n",
    "cities = [\"New York\", \"Los Angeles\", \"Chicago\", \"Houston\", \"Phoenix\", \"Mumbai\", \"Delhi\", \"London\"]\n",
    "statuses = [\"active\", \"inactive\"]\n",
    "\n",
    "# Helper to generate random date\n",
    "def random_date(start, end):\n",
    "    delta = end - start\n",
    "    random_days = random.randint(0, delta.days)\n",
    "    return start + timedelta(days=random_days)\n",
    "\n",
    "# Generate data\n",
    "data = {\n",
    "    \"user_id\": [f\"U{str(i+1).zfill(5)}\" for i in range(num_rows)],\n",
    "    \"signup_date\": [random_date(datetime(2024,1,1), datetime(2024,12,31)) for _ in range(num_rows)],\n",
    "    \"status\": [random.choice(statuses) for _ in range(num_rows)],\n",
    "    \"age\": [random.randint(18, 65) if random.random() > 0.05 else \"\" for _ in range(num_rows)], # 5% missing\n",
    "    \"city\": [random.choice(cities) for _ in range(num_rows)],\n",
    "    \"salary\": [random.randint(20000, 120000) if random.random() > 0.1 else \"N/A\" for _ in range(num_rows)] # 10% missing\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Introduce some invalid dates\n",
    "for i in range(0, num_rows, 10000):\n",
    "    df.loc[i, \"signup_date\"] = \"2024-02-30\"  # invalid date\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv(path /\"large_file.csv\", index=False)\n",
    "print(\"Large CSV generated: large_file.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(path /\"large_file.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For files that don't fit in memory\n",
    "chunk_size = 10000\n",
    "chunks = []\n",
    "\n",
    "for chunk in pd.read_csv(f'{path}large_file.csv', chunksize=chunk_size):\n",
    "    # Process each chunk (filter, transform)\n",
    "    processed = chunk[chunk['status'] == 'active']\n",
    "    chunks.append(processed)\n",
    "\n",
    "# Combine all chunks\n",
    "df_final = pd.concat(chunks, ignore_index=True)\n",
    "print(f\"Total rows after filtering: {len(df_final)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
